{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26abae41-dbb0-490a-9870-8988badc15d2",
   "metadata": {},
   "source": [
    "# Homework 0203 Corpus Statistics\n",
    "## Kyle Walter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1591f404-9c6e-4497-99eb-942d374aaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#import the Packages needed to work with the text\n",
    "import os # will be used to loop through an get all the .txt files\n",
    "import pandas as pd #for working with data frame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # for vectorization\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871ba19e-2550-47ed-80ee-f045bfa58556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '.', ',', \"'\", '!', '-', '\"']\n"
     ]
    }
   ],
   "source": [
    "#added to stopword list punctuation that isn't helpful for the analysis\n",
    "standardStop = stopwords.words('english')\n",
    "addedStop = ['.',',',\"'\",\"!\",\"-\",'\"']\n",
    "for x in addedStop:\n",
    "    standardStop.append(x)\n",
    "print(standardStop) # this prints out the stopword list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb003e-8323-4ca7-a4bb-4bdca2a3d37e",
   "metadata": {},
   "source": [
    "# Reading Text from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9dac1e-41bf-43c2-a6fe-143ba8a09a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Sent    3000 non-null   object\n",
      " 1   text    3000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Creates a list to hold the the text data, then it reads in the text file and splits the line by the tab seperating the sentiment score\n",
    "#Next next it complies the data into a dictinary and converts to a data frame. Lastly printed the DF info to make sure the information is there\n",
    "\n",
    "text = []\n",
    "for item in os.listdir():\n",
    "    if item.endswith('.txt'):\n",
    "        with open(item, 'r') as file:\n",
    "            t = file.readlines()\n",
    "            for item in range(0,len(t)):\n",
    "                content = t[item]\n",
    "                content = content.split('\\t')\n",
    "                content2 = content[1].split('\\n')\n",
    "                dictionary = {'Sent':content2[0], 'text':content[0]}\n",
    "                text.append(dictionary)\n",
    "corpusdf = pd.DataFrame(text)\n",
    "corpusdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "203fde5d-fdf3-44ea-87ca-6beecda87c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1500\n",
       "1    1500\n",
       "Name: Sent, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusdf['Sent'].value_counts() #checked the balance of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940d65f-f235-442e-bcb1-29e5a4949f21",
   "metadata": {},
   "source": [
    "# Reading in Text from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7bfad4-e921-4596-9548-1ab6f16cb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   Sent           1600000 non-null  int64 \n",
      " 1   ID             1600000 non-null  int64 \n",
      " 2   Date of Tweet  1600000 non-null  object\n",
      " 3   Ignore         1600000 non-null  object\n",
      " 4   User           1600000 non-null  object\n",
      " 5   text           1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Reads in and renames the columns from homework 1\n",
    "csvdf = pd.read_csv('twitterSentiment.csv', header = None, encoding='latin-1')\n",
    "csvdf.rename(columns={0:\"Sent\",1:\"ID\",2:\"Date of Tweet\", 3:\"Ignore\", 4:\"User\", 5:\"text\"}, inplace = True)\n",
    "csvdf.info() # inspects the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f863dc0b-7523-4510-99c1-6c256e301062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csvdf.loc[csvdf['Sent']==4,'Sent']=1\n",
    "#csvdf['Sent'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a8a81b-e5ec-40a2-8ded-c5c4ef446057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sent                                               text\n",
       "0     0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1     0  is upset that he can't update his Facebook by ...\n",
       "2     0  @Kenichan I dived many times for the ball. Man...\n",
       "3     0    my whole body feels itchy and like its on fire \n",
       "4     0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#csvdf = csvdf[['Sent','text']]\n",
    "#csvdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda03737-ce0b-4a17-a6b7-ae430a24dd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1603000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   Sent    1603000 non-null  object\n",
      " 1   text    1603000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 36.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#frames = [corpusdf, csvdf]\n",
    "#corpusdf = pd.concat(frames)\n",
    "#corpusdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7909bbf3-ae25-4b33-8347-38f4de52f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del csvdf #free up memory by removing the prior frame no longer in use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85d8e1-afde-47b4-a8e6-c0b93a7445ca",
   "metadata": {},
   "source": [
    "# Tokenization, lemmatize, stopswords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd45ab6-fb9b-4174-b026-744ef3dbbde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.019"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusdf['text']=corpusdf['text'].str.lower() #converts all words to lower case\n",
    "corpusdf['sentence']=corpusdf['text'].apply(lambda x: nltk.sent_tokenize(x)) # Tokenizes by sentence\n",
    "corpusdf['nmbrSentences']=corpusdf['sentence'].apply(lambda x: len(x)) # counts the number of senteces in each corpus\n",
    "np.mean(corpusdf['nmbrSentences']) #check the average length of a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59a6f21-05a8-484c-b774-b9fc9c4e6ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             wow... loved this place.\n",
       "1                                   crust is not good.\n",
       "2            not tasty and the texture was just nasty.\n",
       "3    stopped by during the late may bank holiday of...\n",
       "4    the selection on the menu was great and so wer...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusdf['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5666c1-1a69-45de-a1e5-863013e7688c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', '...', 'loved', 'this', 'place', '.'],\n",
       " ['crust', 'is', 'not', 'good', '.'],\n",
       " ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty', '.'],\n",
       " ['stopped',\n",
       "  'by',\n",
       "  'during',\n",
       "  'the',\n",
       "  'late',\n",
       "  'may',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'off',\n",
       "  'rick',\n",
       "  'steve',\n",
       "  'recommendation',\n",
       "  'and',\n",
       "  'loved',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'selection',\n",
       "  'on',\n",
       "  'the',\n",
       "  'menu',\n",
       "  'was',\n",
       "  'great',\n",
       "  'and',\n",
       "  'so',\n",
       "  'were',\n",
       "  'the',\n",
       "  'prices',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenizedText = [nltk.wordpunct_tokenize(x) for x in corpusdf['text']] #tokenizes the text\n",
    "TokenizedText[:5] #display tokenized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "181c7553-5770-437d-862f-fa7bc4df2e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow ... loved place',\n",
       " 'crust good',\n",
       " 'tasty texture nasty',\n",
       " 'stopped late may bank holiday rick steve recommendation loved',\n",
       " 'selection menu great prices']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removes stop words and connects them back together as Tokens\n",
    "cleanedText = []\n",
    "for sentence in TokenizedText:\n",
    "    cleanedText.append(\" \".join([word for word in sentence if word not in standardStop]))\n",
    "cleanedText[:5] #shows output without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0093a83-4c19-434f-9d51-1890670dec07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['wow', '...', 'loved', 'place'],\n",
       " ['crust', 'good'],\n",
       " ['tasty', 'texture', 'nasty'],\n",
       " ['stopped',\n",
       "  'late',\n",
       "  'may',\n",
       "  'bank',\n",
       "  'holiday',\n",
       "  'rick',\n",
       "  'steve',\n",
       "  'recommendation',\n",
       "  'loved'],\n",
       " ['selection', 'menu', 'great', 'prices']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenizedText = [nltk.wordpunct_tokenize(x) for x in cleanedText] #tokenizes the text\n",
    "TokenizedText[:5] #display tokenized output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446ec549-0521-4477-bcde-1009eaa57123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow,...,loved,place',\n",
       " 'crust,good',\n",
       " 'tasty,texture,nasty',\n",
       " 'stopped,late,may,bank,holiday,rick,steve,recommendation,loved',\n",
       " 'selection,menu,great,price',\n",
       " 'getting,angry,want,damn,pho',\n",
       " 'honeslty,taste,fresh,.)',\n",
       " 'potato,like,rubber,could,tell,made,ahead,time,kept,warmer',\n",
       " 'fry,great',\n",
       " 'great,touch']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer = WordNetLemmatizer() #defines the Lemmantizer\n",
    "lemmaText = [] #creates an empty list for the lemma\n",
    "for sentence in TokenizedText:\n",
    "    lemmaText.append(\",\".join([lemmer.lemmatize(word) for word in sentence]))\n",
    "lemmaText[:10] #displays Lemma Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b12e47ef-a88c-4d4b-961c-94e929ae55a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow,...,love,place',\n",
       " 'crust,good',\n",
       " 'tasti,textur,nasti',\n",
       " 'stop,late,may,bank,holiday,rick,steve,recommend,love',\n",
       " 'select,menu,great,price']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer() #defines Stemmer\n",
    "stemmedText =[] #creates an empty list for the Stemmer\n",
    "for sentence in TokenizedText:\n",
    "    stemmedText.append(\",\".join([stemmer.stem(word) for word in sentence]))\n",
    "stemmedText[:5] #shows stemmer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa04ae2c-22bc-4255-a40e-d9f59a7b13b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  5122\n",
      "Number of Lemma Tokens:  61\n",
      "Number of Stemmed Tokens:  61\n"
     ]
    }
   ],
   "source": [
    "#looking at number of Unique Tokens\n",
    "from itertools import chain\n",
    "Tokens = len(set(chain(*TokenizedText)))\n",
    "lemma = len(set(chain(*lemmaText)))\n",
    "stems = len(set(chain(*stemmedText)))\n",
    "print(\"Number of tokens: \",Tokens)\n",
    "print(\"Number of Lemma Tokens: \", lemma)\n",
    "print(\"Number of Stemmed Tokens: \", stems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6b188bc-a733-46d3-ac64-a61944fd55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeing up membory before moving on to vectorization\n",
    "del stemmedText\n",
    "del lemmaText\n",
    "del TokenizedText\n",
    "del cleanedText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5b3d2-7846-4df0-92fe-85b2387e8995",
   "metadata": {},
   "source": [
    "# Vectorize Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5c7f0d0-74b5-496f-9bc6-d4f833805cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Tokenize for the countvectorization\n",
    "def nltkTokenizer(text):\n",
    "    tokenization = nltk.wordpunct_tokenize(text)\n",
    "    return tokenization\n",
    "#define the CountVectorizer Parameters, it will do the prior precessing of the text\n",
    "#with correct inputs it will also properly take care of items suchs as bigram creation around stopwords\n",
    "vectorizer=CountVectorizer(\n",
    "    lowercase=True, #converts words to lower case\n",
    "    analyzer ='word', #method of counting is at the word/token instead of character\n",
    "    ngram_range=(1,3), #Tokens contain how many grams, in this case between 1 and 3 words\n",
    "    stop_words=standardStop, #sets stopword list to us, in this case from nltk\n",
    "    tokenizer=nltkTokenizer)\n",
    "counts = vectorizer.fit_transform(corpusdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f4e7fa-2ac2-4493-8b87-7ff5bede24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a tuple of the words with their count frequencies\n",
    "sum_words = counts.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbb2cce8-083e-48f2-a9f3-33245b4da10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 230),\n",
       " ('great', 210),\n",
       " ('movie', 182),\n",
       " ('phone', 168),\n",
       " ('film', 163),\n",
       " ('one', 146),\n",
       " ('food', 126),\n",
       " ('like', 125),\n",
       " ('place', 114),\n",
       " ('time', 112),\n",
       " ('service', 108),\n",
       " ('bad', 103),\n",
       " ('really', 103),\n",
       " ('well', 92),\n",
       " ('(', 90),\n",
       " ('would', 87),\n",
       " ('best', 78),\n",
       " ('even', 77),\n",
       " ('ever', 76),\n",
       " ('also', 74)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq[:20] #returns 20 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15330796-6065-40a0-bca8-058abc9596a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('work lost', 1),\n",
       " ('lost regrettable', 1),\n",
       " ('regrettable script', 1),\n",
       " ('unfortunately virtue film', 1),\n",
       " ('virtue film production', 1),\n",
       " ('film production work', 1),\n",
       " ('production work lost', 1),\n",
       " ('work lost regrettable', 1),\n",
       " ('lost regrettable script', 1),\n",
       " ('word embarrassing', 1),\n",
       " ('exceptionally', 1),\n",
       " ('exceptionally bad', 1),\n",
       " ('insult one', 1),\n",
       " ('one intelligence', 1),\n",
       " ('intelligence huge', 1),\n",
       " ('huge waste', 1),\n",
       " ('insult one intelligence', 1),\n",
       " ('one intelligence huge', 1),\n",
       " ('intelligence huge waste', 1),\n",
       " ('huge waste money', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea30698-db66-4bba-8675-739d284e6711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32728"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d9e9c66-80bb-44b4-bb09-305fa242a10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!!</th>\n",
       "      <th>!!!</th>\n",
       "      <th>!!!!</th>\n",
       "      <th>!!!!!</th>\n",
       "      <th>!!!!!!</th>\n",
       "      <th>!!!!!!!!</th>\n",
       "      <th>!!!!!!!!!</th>\n",
       "      <th>!!!!!!!!!!!!!!</th>\n",
       "      <th>!!!!!!!!!!!!!!!!!!!</th>\n",
       "      <th>!!!!!!!!.</th>\n",
       "      <th>...</th>\n",
       "      <th> particularly</th>\n",
       "      <th> particularly good</th>\n",
       "      <th> torture</th>\n",
       "      <th> totally</th>\n",
       "      <th> totally believable</th>\n",
       "      <th> true</th>\n",
       "      <th> true masterpiece</th>\n",
       "      <th></th>\n",
       "      <th> acting</th>\n",
       "      <th> acting especially</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 32728 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      !!  !!!  !!!!  !!!!!  !!!!!!  !!!!!!!!  !!!!!!!!!  !!!!!!!!!!!!!!  \\\n",
       "0      0    0     0      0       0         0          0               0   \n",
       "1      0    0     0      0       0         0          0               0   \n",
       "2      0    0     0      0       0         0          0               0   \n",
       "3      0    0     0      0       0         0          0               0   \n",
       "4      0    0     0      0       0         0          0               0   \n",
       "...   ..  ...   ...    ...     ...       ...        ...             ...   \n",
       "2995   0    0     0      0       0         0          0               0   \n",
       "2996   0    0     0      0       0         0          0               0   \n",
       "2997   0    0     0      0       0         0          0               0   \n",
       "2998   0    0     0      0       0         0          0               0   \n",
       "2999   0    0     0      0       0         0          0               0   \n",
       "\n",
       "      !!!!!!!!!!!!!!!!!!!  !!!!!!!!.  ...   particularly  \\\n",
       "0                       0          0  ...               0   \n",
       "1                       0          0  ...               0   \n",
       "2                       0          0  ...               0   \n",
       "3                       0          0  ...               0   \n",
       "4                       0          0  ...               0   \n",
       "...                   ...        ...  ...             ...   \n",
       "2995                    0          0  ...               0   \n",
       "2996                    0          0  ...               0   \n",
       "2997                    0          0  ...               0   \n",
       "2998                    0          0  ...               0   \n",
       "2999                    0          0  ...               0   \n",
       "\n",
       "       particularly good   torture   totally   totally believable   true  \\\n",
       "0                       0          0          0                     0       0   \n",
       "1                       0          0          0                     0       0   \n",
       "2                       0          0          0                     0       0   \n",
       "3                       0          0          0                     0       0   \n",
       "4                       0          0          0                     0       0   \n",
       "...                   ...        ...        ...                   ...     ...   \n",
       "2995                    0          0          0                     0       0   \n",
       "2996                    0          0          0                     0       0   \n",
       "2997                    0          0          0                     0       0   \n",
       "2998                    0          0          0                     0       0   \n",
       "2999                    0          0          0                     0       0   \n",
       "\n",
       "       true masterpiece     acting   acting especially  \n",
       "0                      0  0         0                    0  \n",
       "1                      0  0         0                    0  \n",
       "2                      0  0         0                    0  \n",
       "3                      0  0         0                    0  \n",
       "4                      0  0         0                    0  \n",
       "...                  ... ..       ...                  ...  \n",
       "2995                   0  0         0                    0  \n",
       "2996                   0  0         0                    0  \n",
       "2997                   0  0         0                    0  \n",
       "2998                   0  0         0                    0  \n",
       "2999                   0  0         0                    0  \n",
       "\n",
       "[3000 rows x 32728 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at Matrix\n",
    "pd.DataFrame(counts.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe06e51-cafb-4a97-ad86-e2df2606d2ac",
   "metadata": {},
   "source": [
    "# Vectorize Term Frequency inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25e0eec-c02c-4ead-a387-4d30a1c884e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfVectorizer = TfidfVectorizer(\n",
    "    lowercase=True, #converts words to lower case\n",
    "    analyzer ='word', #method of counting is at the word/token instead of character\n",
    "    ngram_range=(1,3), #Tokens contain how many grams, in this case between 1 and 3 words\n",
    "    stop_words=standardStop, #sets stopword list to us, in this case from nltk\n",
    "    tokenizer=nltkTokenizer)\n",
    "tfidf = tfVectorizer.fit_transform(corpusdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caeed41f-e3fa-4901-b50b-0121ec74de34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03afb1d9-cd84-4e35-b25d-ffaa74f03f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32728"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6836f58-9608-4331-bc5c-270d023d17ef",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "771d7973-8e2c-4a2c-8913-04ade4b4a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without negation stop words\n",
    "nbModel = MultinomialNB()\n",
    "cv = 10 #set up 10 fold cross validation\n",
    "\n",
    "#Set up dependent and independent Variables\n",
    "countX = counts\n",
    "tfidfX = tfidf\n",
    "y = corpusdf['Sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8172165-6673-41b0-b223-5ad07520c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.81      1500\n",
      "           1       0.82      0.81      0.81      1500\n",
      "\n",
      "    accuracy                           0.81      3000\n",
      "   macro avg       0.81      0.81      0.81      3000\n",
      "weighted avg       0.81      0.81      0.81      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YpredCount = cross_val_predict(nbModel, countX, y, cv=cv)\n",
    "print(metrics.classification_report(y, YpredCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bbe3337-033c-4d99-a1aa-195d16008aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82      1500\n",
      "           1       0.81      0.83      0.82      1500\n",
      "\n",
      "    accuracy                           0.82      3000\n",
      "   macro avg       0.82      0.82      0.82      3000\n",
      "weighted avg       0.82      0.82      0.82      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "YpredTfidf = cross_val_predict(nbModel, tfidfX, y, cv=cv)\n",
    "print(metrics.classification_report(y, YpredTfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5966cc4b-8ebd-4b48-9a35-07b1ea42f3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', \"wouldn't\", '.', ',', \"'\", '!', '-', '\"']\n"
     ]
    }
   ],
   "source": [
    "negations =['no', 'nor', 'not', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn']\n",
    "for x in negations:\n",
    "    standardStop.remove(x)\n",
    "print(standardStop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8804253-f1c4-410b-bd3b-fe50c302c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Negation Stop Words\n",
    "counts = vectorizer.fit_transform(corpusdf['text'])\n",
    "tfidf = tfVectorizer.fit_transform(corpusdf['text'])\n",
    "#Set up dependent and independent Variables\n",
    "countX = counts\n",
    "tfidfX = tfidf\n",
    "y = corpusdf['Sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2312b97-a3be-4c53-85de-7a3add26d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = counts.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec0aee71-ebfd-4d32-8ced-066823875f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 306),\n",
       " ('good', 230),\n",
       " ('great', 210),\n",
       " ('movie', 182),\n",
       " ('phone', 168),\n",
       " ('film', 163),\n",
       " ('one', 146),\n",
       " ('food', 126),\n",
       " ('like', 125),\n",
       " ('place', 114),\n",
       " ('time', 112),\n",
       " ('service', 108),\n",
       " ('bad', 103),\n",
       " ('really', 103),\n",
       " ('well', 92),\n",
       " ('(', 90),\n",
       " ('would', 87),\n",
       " ('no', 83),\n",
       " ('best', 78),\n",
       " ('even', 77)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq[:20] #returns 20 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c65171b3-9a6c-4bf4-832a-20b3b6601025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('work lost', 1),\n",
       " ('lost regrettable', 1),\n",
       " ('regrettable script', 1),\n",
       " ('unfortunately virtue film', 1),\n",
       " ('virtue film production', 1),\n",
       " ('film production work', 1),\n",
       " ('production work lost', 1),\n",
       " ('work lost regrettable', 1),\n",
       " ('lost regrettable script', 1),\n",
       " ('word embarrassing', 1),\n",
       " ('exceptionally', 1),\n",
       " ('exceptionally bad', 1),\n",
       " ('insult one', 1),\n",
       " ('one intelligence', 1),\n",
       " ('intelligence huge', 1),\n",
       " ('huge waste', 1),\n",
       " ('insult one intelligence', 1),\n",
       " ('one intelligence huge', 1),\n",
       " ('intelligence huge waste', 1),\n",
       " ('huge waste money', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_freq[-20:] #20 least frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a434f442-7f2a-4061-9b47-d3e329fc1aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83      1500\n",
      "           1       0.84      0.82      0.83      1500\n",
      "\n",
      "    accuracy                           0.83      3000\n",
      "   macro avg       0.83      0.83      0.83      3000\n",
      "weighted avg       0.83      0.83      0.83      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Prediction with negation words left in for count vectorizer\n",
    "YpredCount = cross_val_predict(nbModel, countX, y, cv=cv)\n",
    "print(metrics.classification_report(y, YpredCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50ef92e5-7961-44b8-94ee-5d4edaae44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.84      1500\n",
      "           1       0.83      0.84      0.84      1500\n",
      "\n",
      "    accuracy                           0.84      3000\n",
      "   macro avg       0.84      0.84      0.84      3000\n",
      "weighted avg       0.84      0.84      0.84      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prediction with Negation words left inf for tfidf vectorizer\n",
    "YpredTfidf = cross_val_predict(nbModel, tfidfX, y, cv=cv)\n",
    "print(metrics.classification_report(y, YpredTfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52ec53-e58b-4ea5-ab27-1120dbd9fe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d634f-a4ca-46ce-8ec0-72508244b288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
