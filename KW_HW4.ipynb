{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the provided dataset, Tokenize, Vectorize and Train an NB Model for both Lie Dection and Sentiment. Find the top 20 most powerful Tokens and compare and contrast if a computer can learn lie dectection similar to what's been previously observed in sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lie</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'Mike\\'s Pizza High Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'i really like this buffet restaurant in Marsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'After I went shopping with some of my friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'Olive Oil Garden was very disappointing. I ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'The Seven Heaven restaurant was never known f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lie sentiment                                             review\n",
       "0   f         n                          'Mike\\'s Pizza High Point\n",
       "1   f         n  'i really like this buffet restaurant in Marsh...\n",
       "2   f         n      'After I went shopping with some of my friend\n",
       "3   f         n  'Olive Oil Garden was very disappointing. I ex...\n",
       "4   f         n  'The Seven Heaven restaurant was never known f..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 load in the data\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"deception_data_converted_final.csv\", usecols=range(3), lineterminator=\"\\n\")\n",
    "#review the data loaded corrected\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n    46\n",
       "p    46\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the balance of sentiment tags\n",
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              'mike's pizza high point\n",
       "1     'i really like this buffet restaurant in marsh...\n",
       "2         'after i went shopping with some of my friend\n",
       "3     'olive oil garden was very disappointing. i ex...\n",
       "4     'the seven heaven restaurant was never known f...\n",
       "                            ...                        \n",
       "87    'pastablities is a locally owned restaurant in...\n",
       "88    'i like the pizza at dominoes for their specia...\n",
       "89    'it was a really amazing japanese restaurant. ...\n",
       "90    'how do i even pick a best experience at joe's...\n",
       "91    'my sister and i ate at this restaurant called...\n",
       "Name: review, Length: 92, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data['review'] = data['review'].str.lower() #convert to lower case\n",
    "data['review'] = data['review'].apply(lambda x: x.replace(\"\\\\\", '')) # removes \\\n",
    "data['review'] #previews the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       ['mike, 's, pizza, high, point]\n",
       "1     [', i, really, like, this, buffet, restaurant,...\n",
       "2     ['after, i, went, shopping, with, some, of, my...\n",
       "3     ['olive, oil, garden, was, very, disappointing...\n",
       "4     ['the, seven, heaven, restaurant, was, never, ...\n",
       "                            ...                        \n",
       "87    ['pastablities, is, a, locally, owned, restaur...\n",
       "88    [', i, like, the, pizza, at, dominoes, for, th...\n",
       "89    ['it, was, a, really, amazing, japanese, resta...\n",
       "90    ['how, do, i, even, pick, a, best, experience,...\n",
       "91    ['my, sister, and, i, ate, at, this, restauran...\n",
       "Name: review, Length: 92, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize\n",
    "import nltk\n",
    "data['review'] = data['review'].apply(lambda x: nltk.word_tokenize(x))\n",
    "data['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "data['lemmaReviews'] = data['review'].apply(lambda x: [lemma.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       ['mike, 's, pizza, high, point]\n",
       "1     [', i, really, like, this, buffet, restaurant,...\n",
       "2     ['after, i, went, shopping, with, some, of, my...\n",
       "3     ['olive, oil, garden, wa, very, disappointing,...\n",
       "4     ['the, seven, heaven, restaurant, wa, never, k...\n",
       "                            ...                        \n",
       "87    ['pastablities, is, a, locally, owned, restaur...\n",
       "88    [', i, like, the, pizza, at, domino, for, thei...\n",
       "89    ['it, wa, a, really, amazing, japanese, restau...\n",
       "90    ['how, do, i, even, pick, a, best, experience,...\n",
       "91    ['my, sister, and, i, ate, at, this, restauran...\n",
       "Name: lemmaReviews, Length: 92, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmaReviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "rWords = stopwords.words('english')\n",
    "rWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords to remove\n",
    "negative =['ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",'mightn',\n",
    " \"mightn't\",\n",
    " 'mustn',\n",
    " \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\",]\n",
    "len(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Stop Word List:  179\n",
      "Updated Stop Words:  144\n"
     ]
    }
   ],
   "source": [
    "#Removing Negation words from Stopword List\n",
    "print(\"Current Stop Word List: \", len(rWords))\n",
    "for word in negative:\n",
    "    rWords.remove(word)\n",
    "print(\"Updated Stop Words: \", len(rWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    46\n",
       "t    46\n",
       "Name: lie, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the balance of lie tags\n",
    "data['lie'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48    'in my favorite restaurant yuenan restaurant. ...\n",
       "86    'blue monkey cafe is my favorite japanese rest...\n",
       "37    'the food was not bad, but the place was all n...\n",
       "20    'usually, i use yelp to find restaurant. the y...\n",
       "72    'stronghearts cafe is the best! the owners hav...\n",
       "                            ...                        \n",
       "17    'i had heard that panera bread is a good place...\n",
       "87    'pastablities is a locally owned restaurant in...\n",
       "64    'gannon’s isle ice cream served the best ice c...\n",
       "3     'olive oil garden was very disappointing. i ex...\n",
       "34    'i once went to a restaurant, which was not ve...\n",
       "Name: review, Length: 92, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reimport the dataframe, TfidfVectorizer will apply the tokenization for me.\n",
    "data = pd.read_csv('deception_data_converted_final.tsv', delimiter='\\t',quoting=3)\n",
    "data['review'] = data['review'].str.lower() #convert to lower case\n",
    "data['review'] = data['review'].apply(lambda x: x.replace(\"\\\\\", '')) # removes \\\n",
    "data = data.sample(n = len(data['lie']),random_state=24) #Randome shuffle of the data through sampling\n",
    "data['review'] #previews the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lie</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>'in my favorite restaurant yuenan restaurant. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'blue monkey cafe is my favorite japanese rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>t</td>\n",
       "      <td>n</td>\n",
       "      <td>'the food was not bad, but the place was all n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>f</td>\n",
       "      <td>n</td>\n",
       "      <td>'usually, i use yelp to find restaurant. the y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>'stronghearts cafe is the best! the owners hav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lie sentiment                                             review\n",
       "48   f         p  'in my favorite restaurant yuenan restaurant. ...\n",
       "86   t         p  'blue monkey cafe is my favorite japanese rest...\n",
       "37   t         n  'the food was not bad, but the place was all n...\n",
       "20   f         n  'usually, i use yelp to find restaurant. the y...\n",
       "72   t         p  'stronghearts cafe is the best! the owners hav..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() #check that the sentiment and lie are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found on Github as sample to incorporate both\n",
    "#nltk.word_tokenizer and Lemmatization for call in Vectorizer\n",
    "#git location: https://gist.github.com/4OH4/f727af7dfc0e6bb0f26d2ea41d89ee55\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`',\"!\",\"?\", \"'\", \"#\",\"@\"]\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in nltk.word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "Tokenizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyles\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'doe', 'ha', \"n't\", 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorize = TfidfVectorizer(\n",
    "    tokenizer=Tokenizer, #applies Tokenization\n",
    "    stop_words=rWords, #removes stopword tokens\n",
    "    ngram_range=(1,2) #creats unigrams, bigrams, and trigrams  \n",
    ")\n",
    "X_train = vectorize.fit_transform(data['review'])\n",
    "y_sent = data['sentiment']\n",
    "y_lie = data['lie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th># winning</th>\n",
       "      <th>$</th>\n",
       "      <th>$ 100</th>\n",
       "      <th>$ 5</th>\n",
       "      <th>$ 6</th>\n",
       "      <th>$ 7</th>\n",
       "      <th>%</th>\n",
       "      <th>% 's</th>\n",
       "      <th>% love</th>\n",
       "      <th>...</th>\n",
       "      <th>yelp find</th>\n",
       "      <th>yelp free</th>\n",
       "      <th>yelp would</th>\n",
       "      <th>york</th>\n",
       "      <th>york city</th>\n",
       "      <th>york known</th>\n",
       "      <th>yuenan</th>\n",
       "      <th>yuenan restaurant</th>\n",
       "      <th>’</th>\n",
       "      <th>’ isle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219666</td>\n",
       "      <td>0.219666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097599</td>\n",
       "      <td>0.08362</td>\n",
       "      <td>0.089421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.119662</td>\n",
       "      <td>0.119662</td>\n",
       "      <td>0.097006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111212</td>\n",
       "      <td>0.111212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 4698 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           #  # winning         $  $ 100       $ 5  $ 6  $ 7    %  % 's  \\\n",
       "0   0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "1   0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "2   0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "3   0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "4   0.119662   0.119662  0.097006    0.0  0.119662  0.0  0.0  0.0   0.0   \n",
       "..       ...        ...       ...    ...       ...  ...  ...  ...   ...   \n",
       "87  0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "88  0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "89  0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "90  0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "91  0.000000   0.000000  0.000000    0.0  0.000000  0.0  0.0  0.0   0.0   \n",
       "\n",
       "    % love  ...  yelp find  yelp free  yelp would     york  york city  \\\n",
       "0      0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "1      0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "2      0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "3      0.0  ...   0.097599        0.0    0.097599  0.08362   0.089421   \n",
       "4      0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "..     ...  ...        ...        ...         ...      ...        ...   \n",
       "87     0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "88     0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "89     0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "90     0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "91     0.0  ...   0.000000        0.0    0.000000  0.00000   0.000000   \n",
       "\n",
       "    york known    yuenan  yuenan restaurant         ’    ’ isle  \n",
       "0          0.0  0.219666           0.219666  0.000000  0.000000  \n",
       "1          0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "2          0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "3          0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "4          0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "..         ...       ...                ...       ...       ...  \n",
       "87         0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "88         0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "89         0.0  0.000000           0.000000  0.111212  0.111212  \n",
       "90         0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "91         0.0  0.000000           0.000000  0.000000  0.000000  \n",
       "\n",
       "[92 rows x 4698 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Sparse Matrix\n",
    "pd.DataFrame(X_train.toarray(), columns=vectorize.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages to build the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "#Cross Validation by 10 folds\n",
    "model = MultinomialNB()\n",
    "lie_pred = cross_val_predict(model, X_train, y_lie, cv=10)\n",
    "sent_pred = cross_val_predict(model, X_train, y_sent, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           f       0.55      0.59      0.57        46\n",
      "           t       0.56      0.52      0.54        46\n",
      "\n",
      "    accuracy                           0.55        92\n",
      "   macro avg       0.55      0.55      0.55        92\n",
      "weighted avg       0.55      0.55      0.55        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_lie,lie_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n       0.82      0.80      0.81        46\n",
      "           p       0.81      0.83      0.82        46\n",
      "\n",
      "    accuracy                           0.82        92\n",
      "   macro avg       0.82      0.82      0.82        92\n",
      "weighted avg       0.82      0.82      0.82        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_sent, sent_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lieModel = model.fit(X_train, y_lie)\n",
    "sentModel = model.fit(X_train, y_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vector, lm, n=20):\n",
    "    feature_names = vector.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(lm.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n+1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-8.5241\t$ 100          \t\t-7.5129\trestaurant     \n",
      "\t-8.5241\t$ 6            \t\t-7.5227\tgreat          \n",
      "\t-8.5241\t$ 7            \t\t-7.5593\tbest           \n",
      "\t-8.5241\t% 's           \t\t-7.5680\tfood           \n",
      "\t-8.5241\t% service      \t\t-7.5740\twa             \n",
      "\t-8.5241\t&              \t\t-7.8170\tamazing        \n",
      "\t-8.5241\t& pop          \t\t-7.8205\tgood           \n",
      "\t-8.5241\t& restaurant   \t\t-7.8904\t's             \n",
      "\t-8.5241\t'after         \t\t-7.8941\tplace          \n",
      "\t-8.5241\t'after reading \t\t-7.9047\tnice           \n",
      "\t-8.5241\t'after went    \t\t-7.9104\tservice        \n",
      "\t-8.5241\t'bill          \t\t-7.9179\tfresh          \n",
      "\t-8.5241\t'bill gate     \t\t-7.9212\tfriendly       \n",
      "\t-8.5241\t'brown         \t\t-7.9241\tfriend         \n",
      "\t-8.5241\t'brown tofu    \t\t-7.9364\talways         \n",
      "\t-8.5241\t'd 'more       \t\t-7.9390\tprice          \n",
      "\t-8.5241\t'friday        \t\t-7.9726\tlike           \n",
      "\t-8.5241\t'friday worse  \t\t-7.9751\tneed           \n",
      "\t-8.5241\t'in diner      \t\t-7.9848\tquality        \n",
      "\t-8.5241\t'just          \t\t-7.9878\tdelicious      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyles\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\kyles\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorize, lieModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-8.5241\t$ 100          \t\t-7.5129\trestaurant     \n",
      "\t-8.5241\t$ 6            \t\t-7.5227\tgreat          \n",
      "\t-8.5241\t$ 7            \t\t-7.5593\tbest           \n",
      "\t-8.5241\t% 's           \t\t-7.5680\tfood           \n",
      "\t-8.5241\t% service      \t\t-7.5740\twa             \n",
      "\t-8.5241\t&              \t\t-7.8170\tamazing        \n",
      "\t-8.5241\t& pop          \t\t-7.8205\tgood           \n",
      "\t-8.5241\t& restaurant   \t\t-7.8904\t's             \n",
      "\t-8.5241\t'after         \t\t-7.8941\tplace          \n",
      "\t-8.5241\t'after reading \t\t-7.9047\tnice           \n",
      "\t-8.5241\t'after went    \t\t-7.9104\tservice        \n",
      "\t-8.5241\t'bill          \t\t-7.9179\tfresh          \n",
      "\t-8.5241\t'bill gate     \t\t-7.9212\tfriendly       \n",
      "\t-8.5241\t'brown         \t\t-7.9241\tfriend         \n",
      "\t-8.5241\t'brown tofu    \t\t-7.9364\talways         \n",
      "\t-8.5241\t'd 'more       \t\t-7.9390\tprice          \n",
      "\t-8.5241\t'friday        \t\t-7.9726\tlike           \n",
      "\t-8.5241\t'friday worse  \t\t-7.9751\tneed           \n",
      "\t-8.5241\t'in diner      \t\t-7.9848\tquality        \n",
      "\t-8.5241\t'just          \t\t-7.9878\tdelicious      \n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorize, sentModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74ecdb5788cb7deb97f5afe576cacc963ac1f43a488ddf01682c97da661b9a1b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
