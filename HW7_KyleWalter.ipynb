{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7_KyleWalter.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7WrBs7UNK1S"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "When working with cleaned and tagged data related to sentiment analysis, the question that can float towards the end is which model to utilize. Models have different strengths and weaknesses and some are better suited to towards different tasks. Some such as a decision tress come with a strong ability to easily explain to a lay audience what the model is doing in the background under the hood. While a a neural network can also create an easy visual the math, especially in complex networks, can quickly go over an audiences head. Moreover if the model needs to explain the why a result is met this may take longer to derive.\n",
        "\n",
        "In text data, due to the high dimensionality that are some models that are commonly used. This is because as dimensionality grows, so to does the time it will take the computer to converge on the final model. Naive Bayes for one, can quickly and easily translate through the high dimensionality and produce a results. Along with Naive Bayes another lazy learner support vector machines model is also able to quickly produce results in various circumstances. The math however does take a bit longer to converge than Naive Bayes. The question is can Support Vector Machines (SVM) outperform Naive Bayes in text sentiment analysis?\n",
        "\n",
        "Over the next several paragraphs and some python code, the two models will be compared. First by vectorizing the text to just unigrams and later with bigrams. Lastly, the SVM will be put through a larger test and see if it can produce better results by adjusting the cost parameter, which also the model to become more generalizable and hopefully perform better on the larger dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GajTh4y0NDYA"
      },
      "source": [
        "#@title Calling of the packages { display-mode: \"form\" }\n",
        "%%capture\n",
        "# import the packages\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download(['stopwords','punkt','wordnet'])\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# explicitly require this experimental feature\n",
        "from sklearn.experimental import enable_halving_search_cv # noqa\n",
        "# now you can import normally from model_selection\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score, train_test_split\n",
        "os.chdir('/content/drive/MyDrive/Graduate School/IST736 Text Mining/Week 7/Week 7 Homework')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPOYCszUcWGF"
      },
      "source": [
        "# The Data\n",
        "\n",
        "The data for the testing tasks comes from a Kaggle Data Set of twitter users who have reviewed about their airline traveling expirence. The Data is already tagged a postive, negative, and neutral.\n",
        "\n",
        "The data set contains a little over 14 thousand tweets for the analysis. The first review was of how many postive, negative, and neutral tweets there were. The Data set is heavily baised toward negative sentiment with 9 thousand tweets balanced towards negative, 3 thousand neutral, and only 2.4 thousand were postive.\n",
        "\n",
        "In order to deal with the imbalance between classes, the data was assigned a weight. 1 for negative, 3 for neutral, and 4 for positive. The data was then sampled using Pandas sample function with replacement and asking for 18,000 records. This will roughly double the number of results for neutral, and positive, while allowing the majority of negative tweets to come through."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "cellView": "form",
        "id": "5hfs688dXaf_",
        "outputId": "50c270e7-84c5-498c-9cb6-29221e0acdc9"
      },
      "source": [
        "#@title Displaying the Data\n",
        "# Import and inspect the data\n",
        "data = pd.read_csv('Tweets.csv', encoding='latin1')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "hDmtN0cKXjvd",
        "outputId": "cd149e07-b146-43c2-9b3c-8f6c24ff5417"
      },
      "source": [
        "#@title Count of Classes Pre-Balancing\n",
        "# Reduce the columns\n",
        "data = data[['text','airline_sentiment']]\n",
        "#Check the Balance of the classes\n",
        "data['airline_sentiment'].value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    9178\n",
              "neutral     3099\n",
              "positive    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "GczjUCawX1W-",
        "outputId": "a3cb88f4-0e18-4a38-c491-f0493e944c97"
      },
      "source": [
        "#@title Count of Classes Post Balancing\n",
        "# Balancing the Data Set\n",
        "weights = {'negative':1, 'neutral':3, 'positive':4} # set the weights\n",
        "data['weights'] = data['airline_sentiment'].apply(lambda x: weights[x])\n",
        "data = data.sample(n = 18000, weights=data['weights'], replace= True, random_state=24)\n",
        "data['airline_sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    6081\n",
              "neutral     5967\n",
              "negative    5952\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UApxFa5RisjD"
      },
      "source": [
        "## Vectorization\n",
        "\n",
        "Since computers can not readily utilize text for analysis, the next step is to vectorize the written text. This process seperates the text into words or short phrases calles Tokens. When a token represents a single word it is called a unigram, 2 words is called a bigram, and so forth through n-grams where n represents the number of words in the token.\n",
        "\n",
        "The Vectorization then can try various methods of representing that word in a corpus, or text body, amoung the larger Corpora of text. There are three commonly seen methods. If the Token exists in the corprus, it will be represented with True or 1 and if it does not then it will be represented with a false. Secondly utilizing counts, which a token will be counted for each time it is represented in the text. Lastly utilizing TFIDF which counts the occurence of a token in a respecitve corpus and divides it by how frequently it is seen across the corpora. This in effect weakens common words while those that are less frequent will stand out.\n",
        "\n",
        "In order to maintain an apples to apples comparison, for all three tasks, the same Tokenization will be applied the Corpora.\n",
        "\n",
        "As Sklearn includes a Vectorization tool that will implement the tokenization, removal of stops, and convert to n-grams in 1 step, the first thing is to define the items that are going to be utilized.\n",
        "\n",
        "First the Tokenization and Lemmatization, or the returning of a word to it's base morpheme so that words like buying and buy will be represented with 1 token buy. This reduces features which text data has large quantities and thus can make model's slow to converge.\n",
        "\n",
        "Using nltk's built in word tokenizer, which keeps punctuation, a number of punctuation characters were removed so as not become their own tokens. In this list specifically the hash and at signs were added as this data set comes from twitter and these may create additional one off tokens.\n",
        "\n",
        "The last vectorization preperatory step was to create a stopword list. Stopwords are common words such as \"the\", \"I\", \"can\" and so on that create sentence structure but often have little meaning in sentiment analysis tasks...except those that negate such \"not\", \"don't\", \"can't\" as English generally requires helper words to do this task rather than morphemes. Nltk's built in stopword list was called and the negation stopwords were removed. These words will be passed to the vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DnHJ5mI1bj4e"
      },
      "source": [
        "#@title Defines Tokenization and Lemmatization in 1 Step\n",
        "#Found on Github as sample to incorporate both\n",
        "#nltk.word_tokenizer and Lemmatization for call in Vectorizer\n",
        "#git location: https://gist.github.com/4OH4/f727af7dfc0e6bb0f26d2ea41d89ee55\n",
        "\n",
        "#Removing the \"#\" and \"@\" sign will remove features specific to twitter.\n",
        "\n",
        "class LemmaTokenizer:\n",
        "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`',\"!\",\"?\", \"'\", \"#\",\"@\"]\n",
        "    def __init__(self):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "    def __call__(self, doc):\n",
        "        return [self.wnl.lemmatize(t) for t in nltk.word_tokenize(doc) if t not in self.ignore_tokens]\n",
        "\n",
        "Tokenizer = LemmaTokenizer()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NzD3P3RtcAJd"
      },
      "source": [
        "#@title Defines Stopwords\n",
        "# Manging the stopword list\n",
        "#import from nltk package\n",
        "stop_words = stopwords.words('english')\n",
        "#gather negative stopwords, english helper words\n",
        "negativeStopwords = ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
        "\"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
        "\"haven't\", 'isn', \"isn't\",'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',\n",
        "\"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\",\n",
        "'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"not\", \"don't\",\"no\",\n",
        "\"nor\",\"don\"]\n",
        "#remove them from the list\n",
        "for word in negativeStopwords:\n",
        " stop_words.remove(word)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7lEBrIsBileL"
      },
      "source": [
        "#@title Creates the defined Vectors for Task 1 and 2\n",
        "#Setting up the vectorizers to use the Tokenizer defined 2 blocks up and the stops words defined in the previous block\n",
        "#by default the Vectorizer will lower case the tokens, and just unigrams\n",
        "#for the Bigrams defined below\n",
        "unigramBinary = CountVectorizer(tokenizer=Tokenizer, stop_words=stop_words, binary=True)\n",
        "unigramCounts = CountVectorizer(tokenizer=Tokenizer, stop_words=stop_words, binary=False)\n",
        "bigramVectorizer = CountVectorizer(tokenizer=Tokenizer, stop_words=stop_words, binary=False, ngram_range=(2,2)) #Only Bigrams\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvQOBZR74VB0"
      },
      "source": [
        "## Training and Test Sets\n",
        "\n",
        "For Task 1 and Task 2, training and test sets will be created using a 60-40 spilt where 60 percent of the data will be used for training and 40 percent will be used to test the model. Sklearn's train_test_split function shuffles the records using psuedo randome. In order for the results to be able to seen by anyone who runs this later a random_state is set that will produce the same set as long as the records are in the same order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "88i6_ILc4RbC"
      },
      "source": [
        "#@title Creates the Test and Training Sets used in Tasks 1 and 2\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['airline_sentiment'], test_size = 0.4, random_state = 37)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ge10kJJ8cHS"
      },
      "source": [
        "Now that the test in training sets are created, the text entries need to be Vectorized. Starting first with the training sets that will training the matrix by using fit_transform method. This will create the features used both sets.\n",
        "\n",
        "Next the test set was passed to the transform where the vectorizer will check for the features and trainsform the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96EOjIYXKMRP"
      },
      "source": [
        "# The Analysis/Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL-5TlUY0szC"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "This task is focused on the comparison of Unigrams Models for both MultinomialNB and Support Vector Machines. For this test the two models were tested below using their base structures.\n",
        "\n",
        "For instance MultinomialNB is instantiated with a defaul smoothing of 1 and the same is done for cost the support vector machines algorithm. The differnt in the two runs is the first utilizes the binary representation of the Token within the Corpus and the later uses the count of how many times the Token appears in the corpus. This was to see if the any changes happen to the model based on the two sets of results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Kf5qlhKP052b"
      },
      "source": [
        "#@title Vectorization of Test and Training for Task 1\n",
        "X_binary = unigramBinary.fit_transform(X_train)\n",
        "X_counts = unigramCounts.fit_transform(X_train)\n",
        "\n",
        "X_binary_test=unigramBinary.transform(X_test)\n",
        "X_counts_test=unigramCounts.transform(X_test)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7xTqpEEZ3M14"
      },
      "source": [
        "#@title Binary Unigram Model\n",
        "#Instantiate the Model\n",
        "nb_clf = MultinomialNB()\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "#Fit the binary models\n",
        "nb_clf.fit(X_binary, y_train)\n",
        "svm_clf.fit(X_binary, y_train)\n",
        "\n",
        "#Create the predictions\n",
        "nb_y_pred = nb_clf.predict(X_binary_test)\n",
        "svm_y_pred = svm_clf.predict(X_binary_test)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "0LcS3lZu9-YQ",
        "outputId": "54547656-cd48-41bc-9b2d-deffe1eb6a61"
      },
      "source": [
        "#@title Multinomial Naive Bayes Results\n",
        "#Checking the results of the MultinomialNB model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, nb_y_pred))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[2082  186  113]\n",
            " [ 451 1686  249]\n",
            " [ 157  115 2161]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.87      0.82      2381\n",
            "     neutral       0.85      0.71      0.77      2386\n",
            "    positive       0.86      0.89      0.87      2433\n",
            "\n",
            "    accuracy                           0.82      7200\n",
            "   macro avg       0.83      0.82      0.82      7200\n",
            "weighted avg       0.83      0.82      0.82      7200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "NIFhReig_3Zk",
        "outputId": "9c6b4f82-23b4-47f4-a041-c41812a306ad"
      },
      "source": [
        "#@title Support Vector Machine's Linear Model Results\n",
        "#Checking the results of the SVM Model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, svm_y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[2004  292   85]\n",
            " [ 190 2058  138]\n",
            " [  71  100 2262]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.88      0.84      0.86      2381\n",
            "     neutral       0.84      0.86      0.85      2386\n",
            "    positive       0.91      0.93      0.92      2433\n",
            "\n",
            "    accuracy                           0.88      7200\n",
            "   macro avg       0.88      0.88      0.88      7200\n",
            "weighted avg       0.88      0.88      0.88      7200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bBvWQukAqO9"
      },
      "source": [
        "The Naive Bayes model based on the results with Binary data shows a good accuracy around 82%. However it struggles with classifiying neutral sentiment. The confusion Matrix shows as well that the model's incorrect prediction tend to be over weighted in the neutral category. Negative Category also show a signficant drop in precision or classifiying newly seen data.\n",
        "\n",
        "The Support Vector Machine also shows the lowest score in the neutral category at .85 but the overal results is much closer to the 88% accuracy of the model. Incorrect predictions are pretty balanced among all three classes. Precision and recall scores a closer to the overall results of the model for all elements. One thing of note the model converges in 10 seconds, this is almost 10x longer than NB model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex5DT-x5FdYh"
      },
      "source": [
        "### The Most indicative Features\n",
        "\n",
        "The advantage of both of these models is that it is easy to pull the most indicative features from them, so long as SVC is run with a linear kernal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "5OJdImijAQ_y",
        "outputId": "845de27f-3cff-4d45-929c-878036feddc2"
      },
      "source": [
        "#@title Most  Indicative Features\n",
        "nb_features = sorted(zip(nb_clf.coef_[0], unigramBinary.get_feature_names_out()), reverse=True)\n",
        "print(\"Top 10 Most Negative Features: \", nb_features[:10])\n",
        "print(\"Top 10 Most Postive Features: \", nb_features[-10:])\n",
        "svm_features = sorted(zip(svm_clf.coef_[0], unigramBinary.get_feature_names_out()), reverse=True)\n",
        "print(svm_features[:10])\n",
        "print(svm_features[-10:])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Most Negative Features:  [(-3.8906343660885954, 'flight'), (-3.9163710807326444, 'united'), (-4.106544612809285, 'usairways'), (-4.191102000837348, 'americanair'), (-4.398741365615592, \"n't\"), (-4.719649085695694, 'southwestair'), (-4.734309034106378, 'no'), (-4.751330721675809, 'not'), (-4.762118312804806, 'wa'), (-4.884249181397293, 'hour')]\n",
            "Top 10 Most Postive Features:  [(-10.895516355801455, \"'til\"), (-10.895516355801455, \"'request\"), (-10.895516355801455, \"'noooo\"), (-10.895516355801455, \"'kewl\"), (-10.895516355801455, \"'just\"), (-10.895516355801455, \"'em\"), (-10.895516355801455, \"'customer\"), (-10.895516355801455, \"'crashing\"), (-10.895516355801455, \"'blumanity\"), (-10.895516355801455, \"'bluemanity\")]\n",
            "[(<1x10348 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 5700 stored elements in Compressed Sparse Row format>, '$')]\n",
            "[(<1x10348 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 5700 stored elements in Compressed Sparse Row format>, '$')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dtcz26ZM9mZ"
      },
      "source": [
        "The top 10 most indactive words for the negative class show words like Flight, United, US Airways, and AmericanAir which are the names of airlines are not very indicative of negative words. Many of the the reviews on twitter. The top 10 negative words does include negations which are words left in from the stopword list.\n",
        "\n",
        "The 10 most indicative words for the positive class appear to be words that are commonly seen in Tweets about Airlines and not words that would normally be considered positive by the population at large. Likely the model would perform poorly when introduced to other types of text data out side of airline tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxVaWlr5SvTD"
      },
      "source": [
        "### Task 1 with Counts\n",
        "\n",
        "Now that the request has been run with Binary true false for tokens with high accuracy, reviewing to see if counts of Token Occurence will provide better returns. For multinomial naive bayes the base model will be utilized. However; since the most indicative features are requested for support vector machines, the linear model will need to be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BNRKPeSvNq88"
      },
      "source": [
        "#@title Counts Unigram Models\n",
        "#Instantiate the Model\n",
        "nb_clf = MultinomialNB()\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "#Fit the count models\n",
        "nb_clf.fit(X_counts, y_train)\n",
        "svm_clf.fit(X_counts, y_train)\n",
        "\n",
        "#Create the predictions\n",
        "nb_y_pred = nb_clf.predict(X_counts_test)\n",
        "svm_y_pred = svm_clf.predict(X_counts_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "UFQknQ-FTLov",
        "outputId": "1eb7047a-ddc3-43b9-fffa-099bb6cbcb8e"
      },
      "source": [
        "#@title Multinomial Model Results\n",
        "#Checking the results of the MultinomialNB model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, nb_y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, nb_y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[2076  189  116]\n",
            " [ 454 1674  258]\n",
            " [ 154  112 2167]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.87      0.82      2381\n",
            "     neutral       0.85      0.70      0.77      2386\n",
            "    positive       0.85      0.89      0.87      2433\n",
            "\n",
            "    accuracy                           0.82      7200\n",
            "   macro avg       0.82      0.82      0.82      7200\n",
            "weighted avg       0.82      0.82      0.82      7200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "59R5rSViTN1h",
        "outputId": "2eaa274d-8afc-4510-bcfa-e66b90600f84"
      },
      "source": [
        "#@title Support Vector Machine Results\n",
        "#Checking the results of the SVM Model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, svm_y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, svm_y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[1991  307   83]\n",
            " [ 187 2070  129]\n",
            " [  69  101 2263]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.89      0.84      0.86      2381\n",
            "     neutral       0.84      0.87      0.85      2386\n",
            "    positive       0.91      0.93      0.92      2433\n",
            "\n",
            "    accuracy                           0.88      7200\n",
            "   macro avg       0.88      0.88      0.88      7200\n",
            "weighted avg       0.88      0.88      0.88      7200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD9VBzdpZp5Y"
      },
      "source": [
        "Based on the model output, counting the token occurence or utilizing binary representation of the Token's existing in the text shows the same results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsfcQBfXT0Ae"
      },
      "source": [
        "#### Most indicative Features\n",
        "\n",
        "As the model's preformance numbers didn't change, reviewing the model's most indicative features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "nqY1OoJzTvs5",
        "outputId": "83746a88-81c2-464c-82a9-bd63fffbf880"
      },
      "source": [
        "#@title Most informative features\n",
        "nb_features = sorted(zip(nb_clf.coef_[0], unigramCounts.get_feature_names_out()), reverse=True)\n",
        "print(nb_features[:10]) #Negative\n",
        "print(nb_features[-10:]) #Positive\n",
        "svm_features = sorted(zip(svm_clf.coef_[0], unigramCounts.get_feature_names_out()), reverse=True)\n",
        "print(svm_features[:10])\n",
        "print(svm_features[-10:])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(-3.7590428114861085, 'flight'), (-3.86741084820789, 'united'), (-4.111864186108851, 'usairways'), (-4.212352584942677, 'americanair'), (-4.346057573609645, \"n't\"), (-4.628199465685811, 'no'), (-4.675333543360264, 'wa'), (-4.7067086659280175, 'not'), (-4.743223878903114, 'southwestair'), (-4.8708694393503755, 'hour')]\n",
            "[(-10.925308785619746, \"'til\"), (-10.925308785619746, \"'request\"), (-10.925308785619746, \"'noooo\"), (-10.925308785619746, \"'kewl\"), (-10.925308785619746, \"'just\"), (-10.925308785619746, \"'em\"), (-10.925308785619746, \"'customer\"), (-10.925308785619746, \"'crashing\"), (-10.925308785619746, \"'blumanity\"), (-10.925308785619746, \"'bluemanity\")]\n",
            "[(<1x10348 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 5666 stored elements in Compressed Sparse Row format>, '$')]\n",
            "[(<1x10348 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 5666 stored elements in Compressed Sparse Row format>, '$')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPsYHILbhZpE"
      },
      "source": [
        "The most imformative 10 features show the same tokens however the order is slightly different. This likely drives why the models with and without counts are showing similiar results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGru11mHhr48"
      },
      "source": [
        "## Task 2 - Bigrams\n",
        "\n",
        "For this task keeping the measures of Task 1 the same with one change in Count Vectorization from unigrams to bigrams and reruning the previous models. The CountVectorization was setup earlier along with the stopword list and vectorization steps. \n",
        "\n",
        "In this round, given there was not a difference seen in the model output via the token counts or boolean value, and since boolean value in a count is assumed at zero, only the counts will be used in the creation of the models.\n",
        "\n",
        "The same training and tests sets defined earlier will remain so these model maintain similar comparables to those in Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6UBOnjXahtac"
      },
      "source": [
        "#@title Bigram Vectorization and Model Execution\n",
        "#Vectorize the text to Bigrams\n",
        "X_train_bigrams = bigramVectorizer.fit_transform(X_train)\n",
        "X_test_bigrams = bigramVectorizer.transform(X_test)\n",
        "\n",
        "#instatiate the models\n",
        "nb_clf = MultinomialNB()\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "#Fit the models\n",
        "nb_clf.fit(X_train_bigrams, y_train)\n",
        "svm_clf.fit(X_train_bigrams, y_train)\n",
        "\n",
        "bigram_ypred_nb = nb_clf.predict(X_test_bigrams)\n",
        "bigram_ypred_svm = svm_clf.predict(X_test_bigrams)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "5XrLuvhfslfS",
        "outputId": "1ae08fb9-acd6-4cd2-b815-983a71c1f08c"
      },
      "source": [
        "#@title Multinomial Model\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, bigram_ypred_nb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, bigram_ypred_nb))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[1975  210  196]\n",
            " [ 239 1902  245]\n",
            " [ 107   93 2233]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.83      0.84      2381\n",
            "     neutral       0.86      0.80      0.83      2386\n",
            "    positive       0.84      0.92      0.87      2433\n",
            "\n",
            "    accuracy                           0.85      7200\n",
            "   macro avg       0.85      0.85      0.85      7200\n",
            "weighted avg       0.85      0.85      0.85      7200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "_xBJKoo62kFb",
        "outputId": "0a6168f4-c886-43fb-855a-a39636e7c5d8"
      },
      "source": [
        "#@title Support Vector Machines\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, bigram_ypred_svm))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, bigram_ypred_svm))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[1731  557   93]\n",
            " [ 122 2186   78]\n",
            " [  60  184 2189]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.73      0.81      2381\n",
            "     neutral       0.75      0.92      0.82      2386\n",
            "    positive       0.93      0.90      0.91      2433\n",
            "\n",
            "    accuracy                           0.85      7200\n",
            "   macro avg       0.86      0.85      0.85      7200\n",
            "weighted avg       0.86      0.85      0.85      7200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SexX_2fa2-th"
      },
      "source": [
        "Interestingly, both model's preform with the same overall accuracy. There is a large misclassifcation of Negative Sentiment as Neurtal in the SVM category. In addition, the while the SVM precision and recall are pretty close in the positive category, both Negative and Neurtal categories have their struggles with well below average recall and precision scores respectively.\n",
        "\n",
        "The multinomial NB is generally more balanced across the board is balances in its results, although as seen in the unigram model it still struggles a bit in the classification of sentiment.\n",
        "\n",
        "That said the best restuls between Classification Task 1 with unigrams and classifcation 2 with Bigrams, the Classification Task 1 with SVM model has so far preformed the best of the all the variation that have been tested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ka2kY6wBSWG"
      },
      "source": [
        "### Most informative Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "9WX3JKbR2nrB",
        "outputId": "ba32596d-7be5-4c58-e09f-70a54e960bd0"
      },
      "source": [
        "#@title Multinomial Features\n",
        "nb_features = sorted(zip(nb_clf.coef_[0], bigramVectorizer.get_feature_names_out()), reverse=True)\n",
        "print(nb_features[:10]) #Negative\n",
        "print(nb_features[-10:])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(-6.088558058226031, '& amp'), (-6.148877848459554, 'cancelled flightled'), (-6.148877848459554, \"ca n't\"), (-6.169821022304797, 'customer service'), (-6.957220741110932, 'late flight'), (-7.148888160323124, 'cancelled flighted'), (-7.237180767468803, 'flight wa'), (-7.317223475142339, \"wo n't\"), (-7.317223475142339, \"n't get\"), (-7.35112502681802, 'late flightr')]\n",
            "[(-11.41156803736444, '$ 440'), (-11.41156803736444, '$ 39'), (-11.41156803736444, '$ 33'), (-11.41156803736444, '$ 25.00'), (-11.41156803736444, '$ 20'), (-11.41156803736444, '$ 192'), (-11.41156803736444, '$ 17.58'), (-11.41156803736444, '$ 1000cost-'), (-11.41156803736444, '$ 1000'), (-11.41156803736444, '$ .50')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "-rPG9XK0FBFj",
        "outputId": "b4630c6f-0dd9-4a86-9091-9ed7a073d508"
      },
      "source": [
        "#@title Support Vector Machine Features\n",
        "svm_features = sorted(zip(svm_clf.coef_[0], bigramVectorizer.get_feature_names_out()), reverse=True)\n",
        "print(svm_features[:10])\n",
        "print(svm_features[-10:])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(<1x48715 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 35590 stored elements in Compressed Sparse Row format>, '$ $')]\n",
            "[(<1x48715 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 35590 stored elements in Compressed Sparse Row format>, '$ $')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPYm9YmAIlqf"
      },
      "source": [
        "Negative features are more indicative of negative events like cancelled flights, can't, customer service (which while not a negative topic, often is only consulted when there is a problem), late flight, etc. \n",
        "\n",
        "The positive indications explain the poorer performance as many of the inidcations show dollar values of what appear to be fees paid in the tweets. None of them are traditionally words thought of as positive in English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBTcRkbDci2j"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "For Task three the ask is to utilize the parameters available for the SVM Model to to see if a better model can be produced. So far in Task 1 utilizing unigrams the best mode has seen an 88% accuracy under SVM, and the best preforming model so far as both recall and percsions are in the mid 80s or higher for all sentiment classifcations.\n",
        "\n",
        "For Task three a combination of both Bigrams and Unigrams were submitted. In addition to try and achieve the best model, SVC's main calcuation Kernel and the Cost were adjusted using a for loop and outputting the classification report for each of the results. This allow various parameters to be tested in 1 run of the code. The computation time is longer, but allows to quickly see which model preforms the best over the code.\n",
        "\n",
        "For SVC the cost was selected range from .5 to 2 in increments of .5 Costs tell the model how tightly or loosely it should classify the training data. Normally speaking lower cost overfits the model to the training data while higher cost allows more generalized results.\n",
        "\n",
        "In addition the other factor tested was the Kernal. Linear and Radial Based Function are both being tested to see if one of them produces a better outcome.\n",
        "\n",
        "To best measure accuracy, cross fold validation is done, where the model is build utilize the majority of the folds and holding 1 out to test the results. This happens for each section of the date and the results is averaged to help identify how well the model is likely to generalize to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "cellView": "form",
        "id": "yZmhYcG2dTP_",
        "outputId": "5cec98b6-1a76-4ff4-86cf-7b2b95fd7e49"
      },
      "source": [
        "#@title SVM Cost and Kernel Comparisons\n",
        "#Create vectorization feature\n",
        "Vectorizer = CountVectorizer(tokenizer=Tokenizer, stop_words=stop_words, binary=False, ngram_range=(1,2))\n",
        "X_vect = Vectorizer.fit_transform(data['text'])\n",
        "y_train_all = data['airline_sentiment']\n",
        "\n",
        "#testing parameters\n",
        "cost = [.5,1,1.5,2]\n",
        "kernal = ['linear', 'rbf']\n",
        "\n",
        "for k in kernal:\n",
        "  for c in cost:\n",
        "    svm_clf = SVC(kernel=k, C=c) #instatiates the model\n",
        "    y_preds = cross_val_predict(svm_clf, X_vect, y_train_all, cv = 5)\n",
        "    print(\"For Cost of: \", c, \"\\n For kernel: \", k, \"\\n Classifcation Report: \\n\", classification_report(y_train_all, y_preds, digits=5))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Cost of:  0.5 \n",
            " For kernel:  linear \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.94035   0.87668   0.90740      5952\n",
            "     neutral    0.88030   0.92559   0.90238      5967\n",
            "    positive    0.94123   0.95609   0.94860      6081\n",
            "\n",
            "    accuracy                        0.91972     18000\n",
            "   macro avg    0.92063   0.91945   0.91946     18000\n",
            "weighted avg    0.92074   0.91972   0.91966     18000\n",
            "\n",
            "For Cost of:  1 \n",
            " For kernel:  linear \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.94016   0.87634   0.90713      5952\n",
            "     neutral    0.88064   0.92609   0.90279      5967\n",
            "    positive    0.94204   0.95691   0.94942      6081\n",
            "\n",
            "    accuracy                        0.92006     18000\n",
            "   macro avg    0.92095   0.91978   0.91978     18000\n",
            "weighted avg    0.92106   0.92006   0.91998     18000\n",
            "\n",
            "For Cost of:  1.5 \n",
            " For kernel:  linear \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.93968   0.87685   0.90718      5952\n",
            "     neutral    0.88139   0.92526   0.90279      5967\n",
            "    positive    0.94144   0.95708   0.94920      6081\n",
            "\n",
            "    accuracy                        0.92000     18000\n",
            "   macro avg    0.92084   0.91973   0.91972     18000\n",
            "weighted avg    0.92095   0.92000   0.91992     18000\n",
            "\n",
            "For Cost of:  2 \n",
            " For kernel:  linear \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.93934   0.87685   0.90702      5952\n",
            "     neutral    0.88146   0.92593   0.90315      5967\n",
            "    positive    0.94220   0.95691   0.94950      6081\n",
            "\n",
            "    accuracy                        0.92017     18000\n",
            "   macro avg    0.92100   0.91990   0.91989     18000\n",
            "weighted avg    0.92112   0.92017   0.92009     18000\n",
            "\n",
            "For Cost of:  0.5 \n",
            " For kernel:  rbf \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.85821   0.86845   0.86330      5952\n",
            "     neutral    0.81896   0.86425   0.84100      5967\n",
            "    positive    0.93363   0.87206   0.90179      6081\n",
            "\n",
            "    accuracy                        0.86828     18000\n",
            "   macro avg    0.87027   0.86825   0.86870     18000\n",
            "weighted avg    0.87068   0.86828   0.86891     18000\n",
            "\n",
            "For Cost of:  1 \n",
            " For kernel:  rbf \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.90248   0.89869   0.90058      5952\n",
            "     neutral    0.87632   0.90364   0.88977      5967\n",
            "    positive    0.95000   0.92485   0.93726      6081\n",
            "\n",
            "    accuracy                        0.90917     18000\n",
            "   macro avg    0.90960   0.90906   0.90920     18000\n",
            "weighted avg    0.90986   0.90917   0.90939     18000\n",
            "\n",
            "For Cost of:  1.5 \n",
            " For kernel:  rbf \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.91109   0.91079   0.91094      5952\n",
            "     neutral    0.89369   0.90866   0.90111      5967\n",
            "    positive    0.95504   0.93965   0.94728      6081\n",
            "\n",
            "    accuracy                        0.91983     18000\n",
            "   macro avg    0.91994   0.91970   0.91978     18000\n",
            "weighted avg    0.92017   0.91983   0.91996     18000\n",
            "\n",
            "For Cost of:  2 \n",
            " For kernel:  rbf \n",
            " Classifcation Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative    0.91208   0.91683   0.91445      5952\n",
            "     neutral    0.90249   0.91051   0.90648      5967\n",
            "    positive    0.95698   0.94376   0.95032      6081\n",
            "\n",
            "    accuracy                        0.92383     18000\n",
            "   macro avg    0.92385   0.92370   0.92375     18000\n",
            "weighted avg    0.92407   0.92383   0.92393     18000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUc_8fJ_eZ-n"
      },
      "source": [
        "The RBF model shows increasing accuracy and pretty stable precision and recall in all three categories. It is worth noting the computation time on the RBF is much longer than the linear. Using cross validation tends to extend the time, but while linear with 10 fold cross validation would take about 2 minutes and 30 seconds to run. RBF was showing around 3 minutes to converge with 10 fold cross validation. The additional 1/5 total time of the linear version in the smaller data set is only helping a few extra corpus get classified with the correct sentiment. However in a larger dataset that time trade off might be well worth the additional time cost.\n",
        "\n",
        "The Linear model does show a very modest increase as cost increases and model bondary becomes more generalized. There is a slight decrease between a cost of 1 and 1.5 in overall accuracy. But that said the model show so little change in overall performance to find it, the result had to be expanded to 5 decimal places to see the change. At lower cost it out preforms the rbf version of the model. However rbf continues to improve as the model gets better. Lastly while both model do show some challenge in differentiating between Negative and Neutral sentiment, linear this gap is wider with the model more likely to classify newly seen data as negative when it is infact neutral.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu_Ps0x9w1Bi"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In the above model tasks 1 and 2, it has been seen that Support Vector Machines preforms as well as Naive Bayes or slightly better. The better however does come at a time time cost; while small, the Naive Bayes generally can build the model is 1.3 seconds and SVM takes around 10 seconds. This expands greatly as seen in task three where cross validation is done is done as part of the model training.\n",
        "\n",
        "Task three, the Kernel different showed that rbf for test data produced better results both in terms of allow the fitted model to become more general, and in the ability to better segregate the three tags.\n",
        "\n"
      ]
    }
  ]
}